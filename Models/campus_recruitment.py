# -*- coding: utf-8 -*-
"""Campus_Recruitment

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iwjiR_MJHXPKXkdiQOsO6gzuVzkH6rbr
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.experimental import enable_halving_search_cv  # Required for HalvingGridSearchCV/HalvingRandomSearchCV
from sklearn.model_selection import RandomizedSearchCV, HalvingGridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score

# Step 1: Load the dataset
data = pd.read_csv('/content/drive/MyDrive/Campus_Recruitment_Data.csv')
data.head()

# Step 2: Exploratory Data Analysis (EDA)
data.info()

data.describe()

# Check for missing values

data.isnull().sum()

# Visualize the distribution of the placement status
sns.countplot(x='PlacementStatus', hue='Internship', data=data, palette="Set1", edgecolor="black")
plt.title('Distribution of Placement Status')
plt.show()

# Drop the columns containing irrelevant information.
columns_to_drop = ['Unnamed: 0', 'StudentId', '12th Percentage', '10th Percentage'] # Added a comma between 'Unnamed: 0' and 'StudentId'
data = data.drop(columns=columns_to_drop)

# Display the updated DataFrame
data.head()

# Visualize correlations between numerical features
numerical_data = data.select_dtypes(include=np.number) # Select only numerical columns
sns.heatmap(numerical_data.corr(), annot=True, cmap='viridis')
plt.title('Correlation Matrix')
plt.show()

X = data.drop('PlacementStatus', axis=1)
y = data['PlacementStatus']

#Checking shape of X
X.shape

# Step 4: Encode categorical features and target variable
# 1. Define categorical, numerical, and salary features
categorical_features = X.select_dtypes(include=['object']).columns.tolist()
numerical_features = X.select_dtypes(exclude=['object']).columns.tolist()

# Remove 'salary' from numerical_features if it's there
if 'salary' in numerical_features:
    numerical_features.remove('salary')

salary_features=['salary']

# 2. Create a custom imputer for the 'salary' column
class SalaryImputer(BaseEstimator, TransformerMixin):
    def __init__(self, fill_value=0):
        self.fill_value = fill_value

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        X = X.copy()
        X['salary'] = X['salary'].fillna(self.fill_value)
        return X

# 3. Create transformers for numerical, salary and categorical features
numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),  # Impute other numerical features with median
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing categorical values
    ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))
])

# 4. Create the ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features),
        ('salary', SalaryImputer(fill_value=0), salary_features) # Custom imputer for 'salary'
    ])

# 5. Create the first pipeline
first_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor)

])

# Step 5: Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

X_train_tr=first_pipeline.fit_transform(X_train)

X_test_tr=first_pipeline.transform(X_test)

# Step 6: Model Selection and Hyperparameter Tuning
models = {
    'Logistic Regression': {
        'model': LogisticRegression(),
        'params': {'logisticregression__C': [0.01, 0.1, 1, 10]}
    },
    'Random Forest': {
        'model': RandomForestClassifier(),
        'params': {
            'randomforestclassifier__n_estimators': np.arange(50, 251, 50),
            'randomforestclassifier__max_depth': [None, 10, 20, 30],
            'randomforestclassifier__min_samples_split': [2, 5, 10],
            'randomforestclassifier__min_samples_leaf': [1, 2, 4]
        }
    },
    'SVM': {
        'model': SVC(probability=True),
        'params': {'svc__C': [0.01, 0.1, 1, 10], 'svc__kernel': ['linear', 'rbf']}
    },
    'KNN': {
        'model': KNeighborsClassifier(),
        'params': {'kneighborsclassifier__n_neighbors': np.arange(3, 11, 2)}
    },
    'Gradient Boosting': {
        'model': GradientBoostingClassifier(),
        'params': {
            'gradientboostingclassifier__n_estimators': np.arange(50, 201, 50),
            'gradientboostingclassifier__learning_rate': [0.01, 0.05, 0.1, 0.2]
        }
    }
}

# Dictionary to store the performance metrics
performance_metrics = {}

# Iterate through each model in the models dictionary
for model_name, model_info in models.items():
    # Create a pipeline with preprocessing and model
    step_name = model_name.lower().replace(" ", "_")
    pipeline = Pipeline(steps=[('preprocessor', preprocessor), (step_name, model_info['model'])])

    # Hyperparameter tuning with GridSearchCV
    # Update parameter names to use step_name
    updated_params = {}
    for param_name, param_values in model_info['params'].items():
        updated_params[f"{step_name}__{param_name.split('__')[1]}"] = param_values

    grid_search = GridSearchCV(pipeline, updated_params, cv=5, scoring='accuracy', n_jobs=-1)
    grid_search.fit(X_train, y_train)

    # Best model and its parameters
    best_model = grid_search.best_estimator_
    print(f"Best parameters for {model_name}: {grid_search.best_params_}")

    # Evaluate the model using the best parameters
    y_pred = best_model.predict(X_test)

    # Calculate performance metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = classification_report(y_test, y_pred, output_dict=True)['Placed']['precision']
    recall = classification_report(y_test, y_pred, output_dict=True)['Placed']['recall']
    f1 = classification_report(y_test, y_pred, output_dict=True)['Placed']['f1-score']

    # Check if the model supports probability estimates for ROC AUC calculation
    if hasattr(best_model.named_steps[step_name], "predict_proba"):
        roc_auc = roc_auc_score(y_test, best_model.predict_proba(X_test)[:, 1])
    else:
        roc_auc = None  # Set to None if the model does not support predict_proba

    # Store performance metrics
    performance_metrics[model_name] = {
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1 Score': f1,
        'ROC AUC': roc_auc
    }

    # Print performance metrics
    print(f"Performance metrics for {model_name}:")
    print(f"  Accuracy: {accuracy:.4f}")
    print(f"  Precision: {precision:.4f}")
    print(f"  Recall: {recall:.4f}")
    print(f"  F1 Score: {f1:.4f}")
    print(f"  ROC AUC: {roc_auc:.4f}" if roc_auc is not None else "  ROC AUC: Not applicable")

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix for {model_name}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

# Step 7: Voting Classifier
voting_clf = VotingClassifier(estimators=[(name, models[name]['model']) for name in models.keys()], voting='soft')
voting_clf.fit(X_train_tr, y_train)

# Evaluate Voting Classifier
y_pred_voting = voting_clf.predict(X_test_tr)
accuracy_voting = accuracy_score(y_test, y_pred_voting)
roc_auc_voting = roc_auc_score(y_test, voting_clf.predict_proba(X_test_tr)[:, 1])
print("Voting Classifier Performance:")
print(f"Accuracy: {accuracy_voting}")
print(f"ROC AUC: {roc_auc_voting}")

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred_voting)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title(f'Confusion Matrix for {voting_clf}')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

